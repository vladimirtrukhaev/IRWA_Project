{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FINAL PROJECT PART 3\n",
    "\n",
    "#### Vladimir Trukhaev & Ingrid Sancho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports \n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import json\n",
    "import collections\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "from array import array\n",
    "import math\n",
    "import numpy as np\n",
    "from numpy import linalg as la\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\vladi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#updating/downloading stop words\n",
    "nltk.download('stopwords')\n",
    "#reading data\n",
    "doc = \"dataset_tweets_WHO.txt\"\n",
    "with open(doc, 'r') as file:\n",
    "    data = json.load(file)\n",
    "    \n",
    "#initializing dictionary \"my_dict\" where value is the tweet text and key its id\n",
    "keylist = []\n",
    "for key in data:\n",
    "    keylist.append(key)\n",
    "\n",
    "my_dict = {}\n",
    "docs_info = {}\n",
    "\n",
    "for i in keylist:\n",
    "    my_dict[i] = None\n",
    "    docs_info[i] = None\n",
    "    \n",
    "for key in data:\n",
    "    #initializing my_dict\n",
    "    tweet = []\n",
    "    for i in data[key][\"full_text\"]:\n",
    "        tweet.append(i)\n",
    "    tweet1 = \"\".join(tweet)\n",
    "    my_dict[key] = tweet1\n",
    "\n",
    "    #creting docs_info\n",
    "    tweet = data[key][\"full_text\"]\n",
    "    username = data[key][\"user\"][\"name\"]\n",
    "    date = data[key][\"created_at\"]\n",
    "    hashtags = data[key][\"entities\"][\"hashtags\"]\n",
    "    likes = data[key][\"favorite_count\"]\n",
    "    retweets = data[key][\"retweet_count\"]\n",
    "    try:\n",
    "        url = data[key][\"entities\"][\"media\"][0][\"expanded_url\"]\n",
    "    except: #sometimes we weren't able to find the url in the data, then:\n",
    "        url = \"https://twitter.com/WHO/status/%s\" % (data[key][\"id_str\"])\n",
    "    \n",
    "    info = {\"tweet\": tweet, \"username\": username, \n",
    "            \"date\": date, \"hashtags\": hashtags, \n",
    "            \"likes\": likes,\"retweets\": retweets, \"url\": url}\n",
    "    docs_info[key] = info\n",
    "    \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parts 1 and 2: Text Processing and Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lowering(d):\n",
    "    \"\"\"\n",
    "    Transforming tweet text (values in dictionary) in lowercase\n",
    "    \n",
    "    Argument:\n",
    "    d -- dictionary where tweets are stored as values\n",
    "    \n",
    "    Returns:\n",
    "    d -- dictionary with tweets transformed to lowercase as values\n",
    "    \"\"\"\n",
    "    for key in d:\n",
    "        d[key] = d[key].lower()\n",
    "    return d\n",
    "\n",
    "def cleaning(d):\n",
    "    \"\"\"\n",
    "    Removing anything that is not alphanumeric\n",
    "    \n",
    "    Argument:\n",
    "    d -- dictionary where tweets are stored as values\n",
    "    \n",
    "    Returns:\n",
    "    d -- dictionary with tweets without any non alphanumeric character\n",
    "    \"\"\"\n",
    "    for key in d:\n",
    "        d[key] = [\"\".join(re.sub(r'[^A-Za-z0-9 #]', ' ', i) for i in d[key])]\n",
    "    return d\n",
    "\n",
    "def tokenize(d):\n",
    "    \"\"\"\n",
    "    Tokenizing the tweets, in other words, splitting text by \"words\"\n",
    "    \n",
    "    Argument:\n",
    "    d -- dictionary where tweets are stored as values\n",
    "    \n",
    "    Returns:\n",
    "    d -- dictionary with lists of words as values\n",
    "    \"\"\"\n",
    "    for key in d:\n",
    "        for sentence in d[key]:\n",
    "            d[key] = sentence.split()\n",
    "    return d\n",
    "\n",
    "def stpwords(d):\n",
    "    \"\"\"\n",
    "    Removing stopwords, which are very common words that do not contain meaning\n",
    "    \n",
    "    Argument:\n",
    "    d -- dictionary where list of words of tweets are stored as values\n",
    "    \n",
    "    Returns:\n",
    "    d -- dictionary with lists of words as values, now with no stopwords\n",
    "    \"\"\"\n",
    "    languages = [\"english\", \"spanish\", \"french\"]\n",
    "    for language in languages:\n",
    "        stop_words = set(stopwords.words(language))\n",
    "        for key in my_dict:\n",
    "            my_dict[key] = [word for word in my_dict[key] if word not in stop_words]\n",
    "    return d\n",
    "\n",
    "def stemming(d):\n",
    "    \"\"\"\n",
    "    Stemming tweets, which means to keep only the \"root\" of each word\n",
    "    \n",
    "    Argument:\n",
    "    d -- dictionary where list of words of tweets are stored as values\n",
    "    \n",
    "    Returns:\n",
    "    d -- dictionary with lists of words as values, now stemmed words\n",
    "    \"\"\"\n",
    "    stemmer = PorterStemmer()\n",
    "    for key in my_dict.keys():\n",
    "        my_dict[key] = [stemmer.stem(word) for word in my_dict[key]]\n",
    "    return d\n",
    "\n",
    "def build_terms(query): #used to process query text\n",
    "    \"\"\"\n",
    "    Preprocess the input query removing stop words, stemming,\n",
    "    transforming in lowercase and return the tokens of the text.\n",
    "    \n",
    "    Argument:\n",
    "    query -- string (text) to be preprocessed\n",
    "    \n",
    "    Returns:\n",
    "    query - a list of tokens corresponding to the input text after the preprocessing\n",
    "    \"\"\"\n",
    "\n",
    "    stemmer = PorterStemmer()\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    query=  query.lower() ## Transform in lowercase\n",
    "    query=  query.split() ## Tokenize the text to get a list of terms\n",
    "    query=[word for word in query if not word in stop_words]  ##eliminate the stopwords\n",
    "    query=[stemmer.stem(word) for word in query] ## perform stemming\n",
    "    \n",
    "    return query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#running every function for our dictionary of tweets \"my_dict\"\n",
    "my_dict = lowering(my_dict)\n",
    "my_dict = cleaning(my_dict)\n",
    "my_dict = tokenize(my_dict)\n",
    "my_dict = stpwords(my_dict)\n",
    "my_dict = stemming(my_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_index_tfidf(my_dict, num_documents):\n",
    "    \"\"\"\n",
    "    Implement the inverted index and compute tf, df and idf\n",
    "    \n",
    "    Argument:\n",
    "    my_dict -- collection of Wikipedia articles\n",
    "    num_documents -- total number of documents\n",
    "    \n",
    "    Returns:\n",
    "    index - the inverted index (implemented through a Pyhon dictionary) containing terms as keys and the corresponding\n",
    "    list of document these keys appears in (and the positions) as values.\n",
    "    tf - normalized term frequency for each term in each document\n",
    "    df - number of documents each term appear in\n",
    "    idf - inverse document frequency of each term\n",
    "    \"\"\"\n",
    "\n",
    "    index = defaultdict(list)\n",
    "    tf = defaultdict(list)  #term frequencies of terms in documents (documents in the same order as in the main index)\n",
    "    df = defaultdict(int)  #document frequencies of terms in the corpus\n",
    "    idf = defaultdict(float)\n",
    "    for doc in my_dict:\n",
    "        current_tweet_index = {}\n",
    "        for position, term in enumerate(my_dict[doc]): # terms contains page_title + page_text. Loop over all terms\n",
    "            try:\n",
    "                  # if the term is already in the index for the current page (current_tweet_index)\n",
    "                  # append the position to the corresponding list\n",
    "                   current_tweet_index[term][1].append(position)  \n",
    "            except:\n",
    "                  # Add the new term as dict key and initialize the array of positions and add the position\n",
    "                    current_tweet_index[term]=[doc, array('I',[position])] #'I' indicates unsigned int (int in Python)\n",
    "              \n",
    "        #normalize term frequencies\n",
    "        # Compute the denominator to normalize term frequencies (formula 2 above)\n",
    "        # norm is the same for all terms of a document.\n",
    "        norm = 0\n",
    "        for term, posting in current_tweet_index.items():\n",
    "            # posting will contain the list of positions for current term in current document. \n",
    "            # posting ==> [current_doc, [list of positions]] \n",
    "            # you can use it to infer the frequency of current term.\n",
    "            norm += len(posting) ** 2\n",
    "        norm = math.sqrt(norm)\n",
    "\n",
    "        #calculate the tf(dividing the term frequency by the above computed norm) and df weights\n",
    "        for term, posting in current_tweet_index.items():\n",
    "            # append the tf for current term (tf = term frequency in current doc/norm)\n",
    "            tf[term].append(np.round(len(posting)/norm,4)) ## SEE formula (1) above\n",
    "            #increment the document frequency of current term (number of documents containing the current term)\n",
    "            df[term] = tf[term] # increment DF for current term\n",
    "\n",
    "        #merge the current page index with the main index\n",
    "        for term_page, posting_page in current_tweet_index.items():\n",
    "            index[term_page].append(posting_page)\n",
    "\n",
    "        # Compute IDF following the formula (3) above. HINT: use np.log\n",
    "        for term in df:\n",
    "            idf[term] = np.round(np.log(float(num_documents/len(df[term]))), 4)\n",
    "\n",
    "    return index, tf, df, idf\n",
    "\n",
    "\n",
    "\n",
    "def rank_documents(terms, docs, index, idf, tf):\n",
    "    \"\"\"\n",
    "    Perform the ranking of the results of a search based on the tf-idf weights\n",
    "    \n",
    "    Argument:\n",
    "    terms -- list of query terms\n",
    "    docs -- list of documents, to rank, matching the query\n",
    "    index -- inverted index data structure\n",
    "    idf -- inverted document frequencies\n",
    "    tf -- term frequencies\n",
    "    \n",
    "    Returns:\n",
    "    Print the list of ranked documents\n",
    "    \"\"\"\n",
    "\n",
    "    # I'm interested only on the element of the docVector corresponding to the query terms \n",
    "    doc_vectors = defaultdict(lambda: [0] * len(terms)) # I call doc_vectors[k] for a nonexistent key k, the key-value pair (k,[0]*len(terms)) will be automatically added to the dictionary\n",
    "    query_vector = [0] * len(terms)\n",
    "\n",
    "    # compute the norm for the query tf\n",
    "    query_terms_count = collections.Counter(terms)  \n",
    "    query_norm = la.norm(list(query_terms_count.values()))\n",
    "\n",
    "    for termIndex, term in enumerate(terms):  #termIndex is the index of the term in the query\n",
    "        if term not in index:\n",
    "            continue\n",
    "  \n",
    "        ## Compute tf*idf(normalize TF as done with documents)\n",
    "        query_vector[termIndex] =  query_terms_count[term]/len(terms)*idf[term]\n",
    "\n",
    "        # Generate doc_vectors for matching docs\n",
    "        for doc_index, (doc, postings) in enumerate(index[term]):            \n",
    "            if doc in docs:\n",
    "                doc_vectors[doc][termIndex] = tf[term][doc_index] * idf[term]  # TODO: check if multiply for idf\n",
    "\n",
    "    # Calculate the score of each doc \n",
    "    # compute the cosine similarity between queyVector and each docVector:\n",
    "    # HINT: you can use the dot product because in case of normalized vectors it corresponds to the cosine similarity\n",
    "    doc_scores=[[np.dot(curDocVec, query_vector), doc] for doc, curDocVec in doc_vectors.items() ]\n",
    "    doc_scores.sort(reverse=True)\n",
    "    result_docs = [x[1] for x in doc_scores]\n",
    "    #print document titles instead if document id's\n",
    "    #result_docs=[ title_index[x] for x in result_docs ]\n",
    "    if len(result_docs) == 0:\n",
    "        print(\"No results found, try again\")\n",
    "        query = input()\n",
    "        docs = search_tf_idf(query, index)\n",
    "    return result_docs\n",
    "\n",
    "def search_tf_idf(query, index):\n",
    "    \"\"\"\n",
    "    output is the list of documents that contain any of the query terms. \n",
    "    So, we will get the list of documents for each query term, and take the union of them.\n",
    "    \"\"\"\n",
    "    query = build_terms(query)\n",
    "    docs = set()\n",
    "    for term in query:\n",
    "        try:\n",
    "            # store in term_docs the ids of the docs that contain \"term\"                        \n",
    "            term_docs=[posting[0] for posting in index[term]]\n",
    "            \n",
    "            # docs = docs Union term_docs\n",
    "            docs = docs.union(term_docs)\n",
    "        except:\n",
    "            #term is not in index\n",
    "            pass\n",
    "    docs = list(docs)\n",
    "    ranked_docs = rank_documents(query, docs, index, idf, tf)\n",
    "    return ranked_docs\n",
    "\n",
    "def rank_query(query):\n",
    "    ranked_docs = search_tf_idf(query, index)\n",
    "    top = 10\n",
    "\n",
    "    print(\"\\n======================\\nTop {} results out of {} for\".format(top, len(ranked_docs)), query,\":\\n\")\n",
    "    for d_id in ranked_docs[:top]:\n",
    "        info_list = [\"tweet\",\"username\",\"date\",\"hashtags\",\"likes\",\"retweets\",\"url\"]\n",
    "        print(\"DOC\", d_id, \"has been retrieved\")\n",
    "        for i in info:\n",
    "            print(i,\":\",docs_info[d_id][i])\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3 TF-IDF with Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time to create the index: 111.38 seconds\n",
      "\n",
      "======================\n",
      "Top 10 results out of 38 for Covid EspaÃ±a ola :\n",
      "\n",
      "DOC 2257 has been retrieved\n",
      "tweet : Q&amp;A #AskWHO on COVID-19 vaccines effectiveness https://t.co/FEdfOREhjn\n",
      "username : World Health Organization (WHO)\n",
      "date : Wed Jun 30 16:12:43 +0000 2021\n",
      "hashtags : [{'text': 'AskWHO', 'indices': [8, 15]}]\n",
      "likes : 219\n",
      "retweets : 85\n",
      "url : https://twitter.com/WHO/status/1410270080873598979\n",
      "\n",
      "\n",
      "DOC 61 has been retrieved\n",
      "tweet : RT @WHOPhilippines: Vaccines canâ€™t stop #COVID19 alone, but by doing it all we can help protect ourselves and our loved ones against COVID-â€¦\n",
      "username : World Health Organization (WHO)\n",
      "date : Mon Oct 11 04:39:10 +0000 2021\n",
      "hashtags : [{'text': 'COVID19', 'indices': [40, 48]}]\n",
      "likes : 0\n",
      "retweets : 71\n",
      "url : https://twitter.com/WHO/status/1447421491428143106\n",
      "\n",
      "\n",
      "DOC 1959 has been retrieved\n",
      "tweet : ğŸ’‰ğŸ’‰ğŸ’‰ğŸ’‰\n",
      "ğŸ’‰ğŸ’‰ğŸ’‰ğŸ’‰\n",
      "ğŸ’‰ğŸ’‰ğŸ’‰ğŸ’‰\n",
      "ğŸ’‰ğŸ’‰ğŸ’‰ğŸ’‰\n",
      "ğŸ’‰ğŸ’‰ğŸ’‰ğŸ’‰                 ğŸ’‰ğŸ’‰ğŸ’‰ğŸ’‰\n",
      "ğŸ’‰ğŸ’‰ğŸ’‰ğŸ’‰                 ğŸ’‰ğŸ’‰ğŸ’‰ğŸ’‰\n",
      "\n",
      "COVID-19 vaccines     COVID-19 vaccines\n",
      "in 10 countries             in the rest of the ğŸŒ\n",
      "\n",
      "#VaccinEquity is ğŸ—ï¸ to ending the pandemic, together!\n",
      "\n",
      "#WorldEmojiDay\n",
      "username : World Health Organization (WHO)\n",
      "date : Sat Jul 17 16:24:23 +0000 2021\n",
      "hashtags : [{'text': 'VaccinEquity', 'indices': [163, 176]}, {'text': 'WorldEmojiDay', 'indices': [218, 232]}]\n",
      "likes : 3486\n",
      "retweets : 1517\n",
      "url : https://twitter.com/WHO/status/1416433609091653633\n",
      "\n",
      "\n",
      "DOC 4 has been retrieved\n",
      "tweet : RT @opsoms: Si estÃ¡ completamente vacunado ğŸ’‰ğŸ’‰, Â¿aÃºn puede contraer COVID-19? \n",
      "\n",
      "ğŸš¨ No importa si estÃ¡ vacunado o si todavÃ­a estÃ¡ esperando, sâ€¦\n",
      "username : World Health Organization (WHO)\n",
      "date : Wed Oct 13 05:47:10 +0000 2021\n",
      "hashtags : []\n",
      "likes : 0\n",
      "retweets : 43\n",
      "url : https://twitter.com/WHO/status/1448163383493136385\n",
      "\n",
      "\n",
      "DOC 1577 has been retrieved\n",
      "tweet : RT @UNICEF: Your guide to breastfeeding safely during the COVID-19 pandemic. \n",
      "\n",
      "#WorldBreastfeedingWeek https://t.co/8EaLDselmn\n",
      "username : World Health Organization (WHO)\n",
      "date : Sun Aug 01 16:29:25 +0000 2021\n",
      "hashtags : [{'text': 'WorldBreastfeedingWeek', 'indices': [79, 102]}]\n",
      "likes : 0\n",
      "retweets : 278\n",
      "url : https://twitter.com/UNICEF/status/1421819251535454211/video/1\n",
      "\n",
      "\n",
      "DOC 1568 has been retrieved\n",
      "tweet : RT @WHOThailand: Our thoughts and prayers are with those who have lost loved ones to COVID-19. https://t.co/bj63wQ6LHu\n",
      "username : World Health Organization (WHO)\n",
      "date : Mon Aug 02 07:25:53 +0000 2021\n",
      "hashtags : []\n",
      "likes : 0\n",
      "retweets : 79\n",
      "url : https://twitter.com/WHOThailand/status/1422064081104642050/photo/1\n",
      "\n",
      "\n",
      "DOC 1169 has been retrieved\n",
      "tweet : RT @DrTedros: Grateful to @LongCovidSOS and other partners who have done so much to improve understanding of post-COVID condition and its iâ€¦\n",
      "username : World Health Organization (WHO)\n",
      "date : Sat Aug 21 18:47:37 +0000 2021\n",
      "hashtags : []\n",
      "likes : 0\n",
      "retweets : 118\n",
      "url : https://twitter.com/WHO/status/1429153230466990083\n",
      "\n",
      "\n",
      "DOC 935 has been retrieved\n",
      "tweet : RT @WHONepal: Nepal has administered more than ğŸ”Ÿmillion doses of COVID-19 ğŸ’‰ğŸ’‰ğŸ’‰, fully vaccinating 15% of its population. This feat wouldn'tâ€¦\n",
      "username : World Health Organization (WHO)\n",
      "date : Thu Sep 02 16:38:12 +0000 2021\n",
      "hashtags : []\n",
      "likes : 0\n",
      "retweets : 101\n",
      "url : https://twitter.com/WHO/status/1433469315710341130\n",
      "\n",
      "\n",
      "DOC 1328 has been retrieved\n",
      "tweet : RT @Ops_Paraguay: ğŸ‘¥La salud comunitaria estÃ¡ en el centro de la vacunaciÃ³n contra la COVID-19 en comunidades indÃ­genas de Paraguay\n",
      "â•INFOğŸ‘‰:â€¦\n",
      "username : World Health Organization (WHO)\n",
      "date : Fri Aug 13 18:01:02 +0000 2021\n",
      "hashtags : []\n",
      "likes : 0\n",
      "retweets : 27\n",
      "url : https://twitter.com/WHO/status/1426242406853353474\n",
      "\n",
      "\n",
      "DOC 200 has been retrieved\n",
      "tweet : ğŸ†• WHO clinical case definition for post #COVID19 condition, also called 'long COVID' https://t.co/WoiLcwsgJX https://t.co/Z0olrHlWPC\n",
      "username : World Health Organization (WHO)\n",
      "date : Thu Oct 07 12:28:10 +0000 2021\n",
      "hashtags : [{'text': 'COVID19', 'indices': [40, 48]}]\n",
      "likes : 723\n",
      "retweets : 373\n",
      "url : https://twitter.com/WHO/status/1446089970758860801/photo/1\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "num_documents = len(my_dict)\n",
    "index, tf, df, idf = create_index_tfidf(my_dict, num_documents)\n",
    "print(\"Total time to create the index: {} seconds\" .format(np.round(time.time() - start_time, 2)))\n",
    "\n",
    "rank_query(\"Covid EspaÃ±a ola\") \n",
    "#rank_query(\"how many covid cases\")\n",
    "#rank_query(\"mortalidad covid\")\n",
    "#rank_query(\"Covid prevention\")\n",
    "#rank_query(\"Pandemia mundial\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3 Our Score with Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "124748\n",
      "38384\n"
     ]
    }
   ],
   "source": [
    "#Calculation total likes and total retweets to normalize values later\n",
    "total_likes = 0\n",
    "total_retweets = 0\n",
    "\n",
    "for i in my_dict:\n",
    "    total_likes += docs_info[\"0\"][\"likes\"]\n",
    "    total_retweets += docs_info[\"0\"][\"retweets\"]\n",
    "    \n",
    "print(total_likes)\n",
    "print(total_retweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_our_score(my_dict, num_documents):\n",
    "    \"\"\"\n",
    "    Implement the inverted index and compute tweet_score\n",
    "    \n",
    "    Argument:\n",
    "    my_dict -- collection of Wikipedia articles\n",
    "    num_documents -- total number of documents\n",
    "    \n",
    "    Returns:\n",
    "    index - the inverted index (implemented through a Pyhon dictionary) containing terms as keys and the corresponding\n",
    "    list of document these keys appears in (and the positions) as values.\n",
    "    tweet_scores - score of each tweet depending on popularity\n",
    "    \"\"\"\n",
    "\n",
    "    index = defaultdict(list)\n",
    "    tweet_score =  {}\n",
    "    \n",
    "    for doc in my_dict:\n",
    "        current_tweet_index = {}\n",
    "        for position, term in enumerate(my_dict[doc]): # terms contains page_title + page_text. Loop over all terms\n",
    "            try:\n",
    "                  # if the term is already in the index for the current page (current_tweet_index)\n",
    "                  # append the position to the corresponding list\n",
    "                   current_tweet_index[term][1].append(position)  \n",
    "            except:\n",
    "                  # Add the new term as dict key and initialize the array of positions and add the position\n",
    "                    current_tweet_index[term]=[doc, array('I',[position])] #'I' indicates unsigned int (int in Python)\n",
    " \n",
    "       \n",
    "        score = {}\n",
    "        for term, posting in current_tweet_index.items():\n",
    "            # Compute tweet score, depending on the importance/popularity of the tweet\n",
    "            likes = docs_info[doc][\"likes\"]\n",
    "            retweets = docs_info[doc][\"retweets\"]\n",
    "            factor = likes/total_likes + retweets / total_retweets\n",
    "            score[term] = 100 * np.round(factor, 4)\n",
    "        \n",
    "        tweet_score[doc] = score\n",
    "\n",
    "        #merge the current page index with the main index\n",
    "        for term_page, posting_page in current_tweet_index.items():\n",
    "            index[term_page].append(posting_page)\n",
    "            \n",
    "    return index, tweet_score\n",
    "\n",
    "\n",
    "\n",
    "def our_rank_documents(terms, docs, index, tweet_score):\n",
    "    \"\"\"\n",
    "    Perform the ranking of the results of a search based on the tweet scores\n",
    "    \n",
    "    Argument:\n",
    "    terms -- list of query terms\n",
    "    docs -- list of documents, to rank, matching the query\n",
    "    index -- inverted index data structure\n",
    "    tweet_score -- inverted document frequencies\n",
    "    \n",
    "    Returns:\n",
    "    The result documents ranked from higher to lower doc_scores\n",
    "    \"\"\"\n",
    "\n",
    "    doc_vectors = defaultdict(lambda: [0] * len(terms)) \n",
    "    query_vector = [0] * len(terms)\n",
    "    query_terms_count = collections.Counter(terms)  \n",
    "    query_norm = la.norm(list(query_terms_count.values()))\n",
    "\n",
    "    for termIndex, term in enumerate(terms):  #termIndex is the index of the term in the query\n",
    "        if term not in index:\n",
    "            continue       \n",
    "\n",
    "        for doc_index, (doc, postings) in enumerate(index[term]):  \n",
    "            query_vector[termIndex] =  query_terms_count[term]*tweet_score[doc][term]/len(terms)\n",
    "            doc_vectors[doc][termIndex] = tweet_score[doc][term]\n",
    "\n",
    "    \n",
    "    doc_scores=[[np.dot(curDocVec, query_vector), doc] for doc, curDocVec in doc_vectors.items() ]\n",
    "    doc_scores.sort(reverse=True)\n",
    "    result_docs = [x[1] for x in doc_scores]\n",
    "    if len(result_docs) == 0:\n",
    "        print(\"No results found, try again\")\n",
    "        query = input()\n",
    "        docs = our_search(query, index)\n",
    "    return result_docs\n",
    "\n",
    "def our_search(query, index):\n",
    "    \"\"\"\n",
    "    output is the list of documents that contain any of the query terms. \n",
    "    So, we will get the list of documents for each query term, and take the union of them.\n",
    "    \"\"\"\n",
    "    query = build_terms(query)\n",
    "    docs = set()\n",
    "    for term in query:\n",
    "        try:                       \n",
    "            term_docs=[posting[0] for posting in index[term]]\n",
    "            docs = docs.union(term_docs)\n",
    "        except:\n",
    "            pass\n",
    "    docs = list(docs)\n",
    "    ranked_docs = our_rank_documents(query, docs, index, tweet_score)\n",
    "    return ranked_docs\n",
    "\n",
    "def our_rank_query(query):\n",
    "    ranked_docs = our_search(query, index)\n",
    "    top = 10\n",
    "\n",
    "    print(\"\\n======================\\nTop {} results out of {} for\".format(top, len(ranked_docs)), query,\":\\n\")\n",
    "    for d_id in ranked_docs[:top]:\n",
    "        info_list = [\"tweet\",\"username\",\"date\",\"hashtags\",\"likes\",\"retweets\",\"url\"]\n",
    "        print(\"DOC\", d_id, \"has been retrieved\")\n",
    "        for i in info_list:\n",
    "            print(i,\":\",docs_info[d_id][i])\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time to create the index: 0.67 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "num_documents = len(my_dict)\n",
    "index, tweet_score = create_our_score(my_dict, num_documents)\n",
    "print(\"Total time to create the index: {} seconds\" .format(np.round(time.time() - start_time, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================\n",
      "Top 10 results out of 38 for Covid EspaÃ±a ola :\n",
      "\n",
      "DOC 1959 has been retrieved\n",
      "tweet : ğŸ’‰ğŸ’‰ğŸ’‰ğŸ’‰\n",
      "ğŸ’‰ğŸ’‰ğŸ’‰ğŸ’‰\n",
      "ğŸ’‰ğŸ’‰ğŸ’‰ğŸ’‰\n",
      "ğŸ’‰ğŸ’‰ğŸ’‰ğŸ’‰\n",
      "ğŸ’‰ğŸ’‰ğŸ’‰ğŸ’‰                 ğŸ’‰ğŸ’‰ğŸ’‰ğŸ’‰\n",
      "ğŸ’‰ğŸ’‰ğŸ’‰ğŸ’‰                 ğŸ’‰ğŸ’‰ğŸ’‰ğŸ’‰\n",
      "\n",
      "COVID-19 vaccines     COVID-19 vaccines\n",
      "in 10 countries             in the rest of the ğŸŒ\n",
      "\n",
      "#VaccinEquity is ğŸ—ï¸ to ending the pandemic, together!\n",
      "\n",
      "#WorldEmojiDay\n",
      "username : World Health Organization (WHO)\n",
      "date : Sat Jul 17 16:24:23 +0000 2021\n",
      "hashtags : [{'text': 'VaccinEquity', 'indices': [163, 176]}, {'text': 'WorldEmojiDay', 'indices': [218, 232]}]\n",
      "likes : 3486\n",
      "retweets : 1517\n",
      "url : https://twitter.com/WHO/status/1416433609091653633\n",
      "\n",
      "\n",
      "DOC 1325 has been retrieved\n",
      "tweet : 3 factors that can help you make safe choices when you're in an area of widespread #COVID19 transmission:\n",
      "\n",
      "1ï¸âƒ£ Location\n",
      "2ï¸âƒ£ Proximity with others\n",
      "3ï¸âƒ£ Time you spend there\n",
      "\n",
      "COVID-19 advice for the public ğŸ‘‰https://t.co/auHlD1QoOX\n",
      "https://t.co/8lAA9M4tOa\n",
      "username : World Health Organization (WHO)\n",
      "date : Sat Aug 14 04:19:49 +0000 2021\n",
      "hashtags : [{'text': 'COVID19', 'indices': [83, 91]}]\n",
      "likes : 1175\n",
      "retweets : 662\n",
      "url : https://twitter.com/WHO/status/1323737698163675138/video/1\n",
      "\n",
      "\n",
      "DOC 200 has been retrieved\n",
      "tweet : ğŸ†• WHO clinical case definition for post #COVID19 condition, also called 'long COVID' https://t.co/WoiLcwsgJX https://t.co/Z0olrHlWPC\n",
      "username : World Health Organization (WHO)\n",
      "date : Thu Oct 07 12:28:10 +0000 2021\n",
      "hashtags : [{'text': 'COVID19', 'indices': [40, 48]}]\n",
      "likes : 723\n",
      "retweets : 373\n",
      "url : https://twitter.com/WHO/status/1446089970758860801/photo/1\n",
      "\n",
      "\n",
      "DOC 931 has been retrieved\n",
      "tweet : Oxygen is an essential medicine, included in the treatment of #COVID19. But not everyone who has COVID-19 will need supplemental oxygen. \n",
      "So, who needs oxygen? \n",
      "Find out more https://t.co/r09i9k0zGl https://t.co/X1ThmUotYk\n",
      "username : World Health Organization (WHO)\n",
      "date : Fri Sep 03 09:57:03 +0000 2021\n",
      "hashtags : [{'text': 'COVID19', 'indices': [62, 70]}]\n",
      "likes : 702\n",
      "retweets : 256\n",
      "url : https://twitter.com/WHO/status/1433730752592687122/photo/1\n",
      "\n",
      "\n",
      "DOC 1963 has been retrieved\n",
      "tweet : ğŸ†• report confirms that #HIV infection is a significant independent risk factor for both severe/critical #COVID19 presentation at hospital admission &amp; in-hospital mortality. 23.1% of all people living with HIV who were hospitalized with COVID-19, died.\n",
      "\n",
      "ğŸ‘‰ https://t.co/UBd0OyFKnd https://t.co/UHLo9RPa1g\n",
      "username : World Health Organization (WHO)\n",
      "date : Fri Jul 16 11:54:56 +0000 2021\n",
      "hashtags : [{'text': 'HIV', 'indices': [23, 27]}, {'text': 'COVID19', 'indices': [104, 112]}]\n",
      "likes : 409\n",
      "retweets : 211\n",
      "url : https://twitter.com/WHO/status/1416003415138873347/photo/1\n",
      "\n",
      "\n",
      "DOC 1849 has been retrieved\n",
      "tweet : COVID-19 vaccine inequity is undermining ğŸŒ economic recovery. ğŸ†•WHO, @UNDP and @BlavatnikSchool Dashboard on #VaccinEquity finds that low-income countries would add $38B to their GDP forecast if they had the same vaccination rate as high-income countries\n",
      "https://t.co/BsYF2eYL1G\n",
      "username : World Health Organization (WHO)\n",
      "date : Thu Jul 22 13:43:53 +0000 2021\n",
      "hashtags : [{'text': 'VaccinEquity', 'indices': [108, 121]}]\n",
      "likes : 383\n",
      "retweets : 168\n",
      "url : https://twitter.com/WHO/status/1418205159188955137\n",
      "\n",
      "\n",
      "DOC 904 has been retrieved\n",
      "tweet : #COVID19 variants &amp; vaccines:\n",
      "\n",
      "âœ… COVID-19 vaccines provide strong protection against serious illness &amp; death\n",
      "âœ… Get all necessary doses to develop maximum protection\n",
      "âœ… Continue practicing all the protective behaviours even after vaccination to stop COVID-19 variants\n",
      "username : World Health Organization (WHO)\n",
      "date : Mon Sep 06 08:09:59 +0000 2021\n",
      "hashtags : [{'text': 'COVID19', 'indices': [0, 8]}]\n",
      "likes : 352\n",
      "retweets : 169\n",
      "url : https://twitter.com/WHO/status/1434790971632336906\n",
      "\n",
      "\n",
      "DOC 1577 has been retrieved\n",
      "tweet : RT @UNICEF: Your guide to breastfeeding safely during the COVID-19 pandemic. \n",
      "\n",
      "#WorldBreastfeedingWeek https://t.co/8EaLDselmn\n",
      "username : World Health Organization (WHO)\n",
      "date : Sun Aug 01 16:29:25 +0000 2021\n",
      "hashtags : [{'text': 'WorldBreastfeedingWeek', 'indices': [79, 102]}]\n",
      "likes : 0\n",
      "retweets : 278\n",
      "url : https://twitter.com/UNICEF/status/1421819251535454211/video/1\n",
      "\n",
      "\n",
      "DOC 1561 has been retrieved\n",
      "tweet : If you have recovered from #COVID19 but are still experiencing certain symptoms you could have post COVID-19 condition or \"long COVID\". What are these symptoms? How long do they last and are there any treatment options? Dr @diazjv explains in #ScienceIn5 â¬‡ï¸ https://t.co/vtDiBhZsJE\n",
      "username : World Health Organization (WHO)\n",
      "date : Mon Aug 02 11:38:39 +0000 2021\n",
      "hashtags : [{'text': 'COVID19', 'indices': [27, 35]}, {'text': 'ScienceIn5', 'indices': [243, 254]}]\n",
      "likes : 277\n",
      "retweets : 154\n",
      "url : https://twitter.com/WHO/status/1422159909957869572/video/1\n",
      "\n",
      "\n",
      "DOC 1310 has been retrieved\n",
      "tweet : Getting vaccinated against #COVID19 helps protect you from getting sick. As soon as it's your turn, take your vaccine!\n",
      "\n",
      "All approved COVID-19 vaccines have been thoroughly tested, and all provide a high degree of protection against getting seriously ill &amp; dying from the disease. https://t.co/bYpU3WvpuM\n",
      "username : World Health Organization (WHO)\n",
      "date : Sun Aug 15 06:01:21 +0000 2021\n",
      "hashtags : [{'text': 'COVID19', 'indices': [27, 35]}]\n",
      "likes : 299\n",
      "retweets : 120\n",
      "url : https://twitter.com/WHO/status/1426786068192366592/video/1\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "our_rank_query(\"Covid EspaÃ±a ola\")\n",
    "#our_rank_query(\"how many covid cases\")\n",
    "#our_rank_query(\"mortalidad covid\")\n",
    "#our_rank_query(\"Covid prevention\")\n",
    "#our_rank_query(\"Pandemia mundial\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3 Word2Vec Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 6135\n"
     ]
    }
   ],
   "source": [
    "#creating the word2vec model for all words\n",
    "model = Word2Vec.load(\"word2vec.model\")\n",
    "data = list(my_dict.values())\n",
    "model = Word2Vec(data, min_count=1,workers=3, window =3, sg = 1)\n",
    "print('Vocabulary size:', len(model.wv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.17750047  0.2753453   0.07210252  0.09172665 -0.0166772  -0.24623951\n",
      " -0.01476946  0.42040315 -0.07766458 -0.26529142 -0.02193881 -0.36936194\n",
      "  0.02012352 -0.07556673  0.00131207 -0.1160517   0.1481008  -0.19787352\n",
      " -0.10737696 -0.47864127 -0.03075087  0.02236611  0.18150647 -0.14117697\n",
      " -0.15420394  0.190576   -0.22254181 -0.09717412  0.05581233 -0.096796\n",
      "  0.22647312  0.11482931  0.08320782 -0.06375208 -0.08854322  0.07898424\n",
      "  0.02796204 -0.07855964 -0.12936035 -0.32306537  0.14464046 -0.25103387\n",
      " -0.13584764 -0.02176412  0.06752764 -0.02452244 -0.08505324  0.10263211\n",
      "  0.04678888  0.24983966  0.14914338 -0.22693233 -0.15400195  0.08527599\n",
      " -0.3661061   0.07176106  0.07126559 -0.22174682 -0.2969128   0.17361921\n",
      " -0.02904036  0.11960578  0.03548748 -0.03464152 -0.3053323   0.10955701\n",
      "  0.12347759  0.2815211  -0.16700931  0.29589415 -0.19407175  0.14546491\n",
      "  0.28786665 -0.16540863  0.36104494  0.0437502  -0.12143011 -0.03212233\n",
      " -0.14780216 -0.02381637 -0.29838827 -0.0263265  -0.27274513  0.2631091\n",
      "  0.04833125 -0.02246596  0.12159879  0.1036537   0.15520535 -0.01048898\n",
      "  0.4633687   0.09240776 -0.01204058  0.14808933  0.39942002  0.18328184\n",
      "  0.12279096 -0.33933574 -0.02687498  0.05522147]\n"
     ]
    }
   ],
   "source": [
    "print(model.wv[\"covid\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function returning a \n",
    "tweet2vec = {}\n",
    "def doc_2_vec(my_dict):\n",
    "    \"\"\"\n",
    "    Creating a dictionary with each tweet's vector reperesentation from the word2vec model\n",
    "    \n",
    "    Argument:\n",
    "    my_dict -- dictioanary of list of tearm, previously created and used\n",
    "    \n",
    "    Returns:\n",
    "    tweet2vec -- dictionary with doc id as key and vector representation as value\n",
    "    \"\"\"\n",
    "    for doc in my_dict:\n",
    "        embeddings = []\n",
    "        tweet_vec = []\n",
    "        for token in doc:\n",
    "            if token in model.wv:\n",
    "                embeddings.append(model.wv.word_vec(token))\n",
    "            else:\n",
    "                embeddings.append(np.random.rand(100))\n",
    "            # averaging all the vectors of individual words to get the vector of the tweet\n",
    "        tweet_vec = np.mean(embeddings, axis=0)\n",
    "        tweet2vec[doc] = tweet_vec\n",
    "    return tweet2vec\n",
    "\n",
    "#Function returning vector resprsentation of a query\n",
    "def query_2_vec(query):\n",
    "    \"\"\"\n",
    "    Creating the query's vector reperesentation from the word2vec model\n",
    "    \n",
    "    Argument:\n",
    "    query -- input terms for which we what to search \n",
    "    \n",
    "    Returns:\n",
    "    query2vec -- vector representation of the query\n",
    "    \"\"\"\n",
    "    embeddings = []\n",
    "    for token in query:\n",
    "        if token in model.wv:\n",
    "            embeddings.append(model.wv.word_vec(token))\n",
    "        else:\n",
    "            embeddings.append(np.random.rand(100))\n",
    "        # averaging all the vectors of individual words to get the vector of the tweet\n",
    "    query2vec = np.mean(embeddings, axis=0)\n",
    "    return query2vec\n",
    "\n",
    "#in order to compute cosine similarity on non normalized vectors\n",
    "from gensim import matutils\n",
    "def similarity_cosine(vec1, vec2):\n",
    "    cosine_similarity = np.dot(matutils.unitvec(vec1), matutils.unitvec(vec2))\n",
    "    return cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_20(q):\n",
    "    \"\"\"\n",
    "    This function first processes teh query and calls to the query vector represenattion function.\n",
    "    Then it computes the cosine similarity on the tweets vectors and the query vector.\n",
    "    Finaly, it prints the 20 most similar tweets.\n",
    "    \n",
    "    Argument:\n",
    "    q -- the query, for which we what to search \n",
    "    \n",
    "    \"\"\"\n",
    "    similarity = []\n",
    "    top = 20\n",
    "    query = build_terms(q) #text processing the query\n",
    "    query2vec = query_2_vec(query) #converting query to vector\n",
    "    for doc in tweet2vec:\n",
    "        similarity.append([similarity_cosine(tweet2vec[doc] , query2vec), doc])\n",
    "        \n",
    "    similarity.sort(reverse=True)\n",
    "    result_docs = [x[1] for x in similarity]\n",
    "    \n",
    "    if len(result_docs) == 0:\n",
    "        print(\"No results found, try again\")\n",
    "        query = input()\n",
    "        docs = our_search(query, index)\n",
    "   \n",
    "    else:\n",
    "        print(\"\\n======================\\nTop {} results out of {} for\".format(top, len(result_docs)), q,\":\\n\")\n",
    "        for d_id in result_docs[:top]:\n",
    "            info_list = [\"tweet\",\"username\",\"date\",\"hashtags\",\"likes\",\"retweets\",\"url\"]\n",
    "            print(\"DOC\", d_id, \"has been retrieved\")\n",
    "            for i in info_list:\n",
    "                print(i,\":\",docs_info[d_id][i])\n",
    "            print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-14-83d1c0f9cb92>:18: DeprecationWarning: Call to deprecated `word_vec` (Use get_vector instead).\n",
      "  embeddings.append(model.wv.word_vec(token))\n"
     ]
    }
   ],
   "source": [
    "#creating the dictionary of tweet vectors\n",
    "tweet2vec = doc_2_vec(my_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================\n",
      "Top 20 results out of 2399 for Covid EspaÃ±a ola :\n",
      "\n",
      "DOC 99 has been retrieved\n",
      "tweet : If someone you know has #depression, here is how you can help:\n",
      "ğŸ’™ Encourage regular eating, sleeping and exercise\n",
      "ğŸ’™ Encourage them to focus on the positive, rather than the negative\n",
      "ğŸ’™ Be patient - recovery can take time\n",
      "\n",
      "#WorldMentalHealthDay #LetsTalk https://t.co/u92OcTVT94\n",
      "username : World Health Organization (WHO)\n",
      "date : Sun Oct 10 07:16:08 +0000 2021\n",
      "hashtags : [{'text': 'depression', 'indices': [24, 35]}, {'text': 'WorldMentalHealthDay', 'indices': [220, 241]}, {'text': 'LetsTalk', 'indices': [242, 251]}]\n",
      "likes : 539\n",
      "retweets : 246\n",
      "url : https://twitter.com/WHO/status/1447098609447669761/photo/1\n",
      "\n",
      "\n",
      "DOC 9 has been retrieved\n",
      "tweet : RT @DrTedros: Broad administration of booster doses is unfair, unjust &amp; immoral at a time when #healthworkers &amp; at most risk people in manyâ€¦\n",
      "username : World Health Organization (WHO)\n",
      "date : Tue Oct 12 21:01:40 +0000 2021\n",
      "hashtags : [{'text': 'healthworkers', 'indices': [99, 113]}]\n",
      "likes : 0\n",
      "retweets : 127\n",
      "url : https://twitter.com/WHO/status/1448031137171968000\n",
      "\n",
      "\n",
      "DOC 999 has been retrieved\n",
      "tweet : @DrTedros \"The WHO Hub for Pandemic &amp; Epidemic Intelligence is about: leveraging innovations in data science, harnessing the power of AI, quantum computing &amp; other cutting-edge technologies, &amp; fostering greater sharing of data &amp; information, between communities &amp; countries\"-@DrTedros\n",
      "username : World Health Organization (WHO)\n",
      "date : Wed Sep 01 14:27:21 +0000 2021\n",
      "hashtags : []\n",
      "likes : 59\n",
      "retweets : 20\n",
      "url : https://twitter.com/WHO/status/1433073998120857601\n",
      "\n",
      "\n",
      "DOC 998 has been retrieved\n",
      "tweet : @DrTedros \"No single institution or nation can do this alone. Thatâ€™s why we have coined the term â€œcollaborative intelligenceâ€ to sum up our collective mission. This Hub will bring together scientists, innovators, policy makers, &amp; civil society representatives from around the ğŸŒ\"-@DrTedros\n",
      "username : World Health Organization (WHO)\n",
      "date : Wed Sep 01 14:27:57 +0000 2021\n",
      "hashtags : []\n",
      "likes : 55\n",
      "retweets : 10\n",
      "url : https://twitter.com/WHO/status/1433074150118203392\n",
      "\n",
      "\n",
      "DOC 989 has been retrieved\n",
      "tweet : @DrTedros @Chikwe_I @schwartlanderb \"No one has done more to make the vision of the WHO Hub a reality than Chancellor Merkel. Under her leadership, ğŸ‡©ğŸ‡ª has become a leading advocate for global health. This is not a recent development, or a sudden realisation that health matters in the wake of a pandemic\"-@DrTedros\n",
      "username : World Health Organization (WHO)\n",
      "date : Wed Sep 01 14:32:40 +0000 2021\n",
      "hashtags : []\n",
      "likes : 46\n",
      "retweets : 10\n",
      "url : https://twitter.com/WHO/status/1433075338775564290\n",
      "\n",
      "\n",
      "DOC 899 has been retrieved\n",
      "tweet : Lack of accessible health information is one of the main barriers excluding people with disabilities from accessing &amp; receiving everyday healthcare services.\n",
      "\n",
      "ğŸ‘‰https://t.co/Qrp2lez1JC #HealthForAll https://t.co/ILqZY83EPf\n",
      "username : World Health Organization (WHO)\n",
      "date : Tue Sep 07 02:19:15 +0000 2021\n",
      "hashtags : [{'text': 'HealthForAll', 'indices': [188, 201]}]\n",
      "likes : 158\n",
      "retweets : 59\n",
      "url : https://twitter.com/WHO/status/1435065096514912262/video/1\n",
      "\n",
      "\n",
      "DOC 89 has been retrieved\n",
      "tweet : Worried that someone you know may be considering #suicide?\n",
      " \n",
      "#LetsTalk about what you should know and how you can show that you care. ğŸ’›\n",
      " \n",
      "#WorldMentalHealthDay https://t.co/w4vvecv3Ji\n",
      "username : World Health Organization (WHO)\n",
      "date : Sun Oct 10 11:54:47 +0000 2021\n",
      "hashtags : [{'text': 'suicide', 'indices': [49, 57]}, {'text': 'LetsTalk', 'indices': [61, 70]}, {'text': 'WorldMentalHealthDay', 'indices': [138, 159]}]\n",
      "likes : 257\n",
      "retweets : 99\n",
      "url : https://twitter.com/WHO/status/1447168732938350599/photo/1\n",
      "\n",
      "\n",
      "DOC 98 has been retrieved\n",
      "tweet : If someone you know has #depression, remember:\n",
      " \n",
      "ğ˜ğ˜¯ ğ˜°ğ˜³ğ˜¥ğ˜¦ğ˜³ ğ˜µğ˜° ğ˜´ğ˜¶ğ˜±ğ˜±ğ˜°ğ˜³ğ˜µ ğ˜°ğ˜µğ˜©ğ˜¦ğ˜³ğ˜´, ğ˜ºğ˜°ğ˜¶ ğ˜¯ğ˜¦ğ˜¦ğ˜¥ ğ˜µğ˜° ğ˜µğ˜¢ğ˜¬ğ˜¦ ğ˜¤ğ˜¢ğ˜³ğ˜¦ ğ˜°ğ˜§ ğ˜ºğ˜°ğ˜¶ğ˜³ğ˜´ğ˜¦ğ˜­ğ˜§ ğ˜µğ˜°ğ˜°. ğŸ’™\n",
      " \n",
      "#WorldMentalHealthDay #LetsTalk https://t.co/6Cm7FfxVgz\n",
      "username : World Health Organization (WHO)\n",
      "date : Sun Oct 10 07:30:22 +0000 2021\n",
      "hashtags : [{'text': 'depression', 'indices': [24, 35]}, {'text': 'WorldMentalHealthDay', 'indices': [120, 141]}, {'text': 'LetsTalk', 'indices': [142, 151]}]\n",
      "likes : 472\n",
      "retweets : 199\n",
      "url : https://twitter.com/WHO/status/1447102187839033347/photo/1\n",
      "\n",
      "\n",
      "DOC 889 has been retrieved\n",
      "tweet : RT @DrTedros: #AirPollution kills 7 million people every year. On #WorldCleanAirDay, let's recommit to accelerate action towards clean airâ€¦\n",
      "username : World Health Organization (WHO)\n",
      "date : Tue Sep 07 14:46:18 +0000 2021\n",
      "hashtags : [{'text': 'AirPollution', 'indices': [14, 27]}, {'text': 'WorldCleanAirDay', 'indices': [66, 83]}]\n",
      "likes : 0\n",
      "retweets : 83\n",
      "url : https://twitter.com/WHO/status/1435253098117836806\n",
      "\n",
      "\n",
      "DOC 988 has been retrieved\n",
      "tweet : @DrTedros @Chikwe_I @schwartlanderb \"Health has been a central theme of Chancellor Merkelâ€™s leadership throughout her tenure. This began with the 2007 G8-Summit in Heiligendamm, which mobilized 60 billion US dollars for global health\"-@DrTedros https://t.co/81QBerkmCY\n",
      "username : World Health Organization (WHO)\n",
      "date : Wed Sep 01 14:33:00 +0000 2021\n",
      "hashtags : []\n",
      "likes : 36\n",
      "retweets : 15\n",
      "url : https://twitter.com/WHO/status/1433075420543471618\n",
      "\n",
      "\n",
      "DOC 898 has been retrieved\n",
      "tweet : It's the International Day of Clean Air for Blue Skies\n",
      " \n",
      "#AirPollution kills an estimated 7âƒ£ million people every year around the ğŸŒğŸŒğŸŒ\n",
      " \n",
      "To save lives and achieve #HealthForAll, let's align our efforts and claim the right to clean air. https://t.co/YrbJkvjOUT\n",
      "username : World Health Organization (WHO)\n",
      "date : Tue Sep 07 05:45:52 +0000 2021\n",
      "hashtags : [{'text': 'AirPollution', 'indices': [57, 70]}, {'text': 'HealthForAll', 'indices': [162, 175]}]\n",
      "likes : 1107\n",
      "retweets : 455\n",
      "url : https://twitter.com/WHO/status/1435117089723928579/video/1\n",
      "\n",
      "\n",
      "DOC 88 has been retrieved\n",
      "tweet : If you think someone may be considering #suicide, remember:\n",
      "ğŸ”¸Many people think about suicide at some point in their lives\n",
      "ğŸ”¸Suicidal thoughts and behaviours are signs of severe emotional distress - not weakness\n",
      "ğŸ”¸It is possible to get better\n",
      "\n",
      "#WorldMentalHealthDay https://t.co/t2qCQornM8\n",
      "username : World Health Organization (WHO)\n",
      "date : Sun Oct 10 12:29:09 +0000 2021\n",
      "hashtags : [{'text': 'suicide', 'indices': [40, 48]}, {'text': 'WorldMentalHealthDay', 'indices': [241, 262]}]\n",
      "likes : 371\n",
      "retweets : 203\n",
      "url : https://twitter.com/WHO/status/1447177381001764866/photo/1\n",
      "\n",
      "\n",
      "DOC 8 has been retrieved\n",
      "tweet : RT @DrTedros: #COVID19 has had a major impact on peopleâ€™s #mentalhealth. Thanks, @DoctorJas of @WHOPhilippines, for your dedicated work toâ€¦\n",
      "username : World Health Organization (WHO)\n",
      "date : Tue Oct 12 21:01:45 +0000 2021\n",
      "hashtags : [{'text': 'COVID19', 'indices': [14, 22]}, {'text': 'mentalhealth', 'indices': [58, 71]}]\n",
      "likes : 0\n",
      "retweets : 35\n",
      "url : https://twitter.com/WHO/status/1448031156348362754\n",
      "\n",
      "\n",
      "DOC 888 has been retrieved\n",
      "tweet : More than 600 million people fall ill every year from eating food contaminated with bacteria, viruses, parasites, toxins or chemicals. \n",
      "Everyone has a role to play to keep food safe https://t.co/xPN75h2GdR https://t.co/ozDeAbT8kM\n",
      "username : World Health Organization (WHO)\n",
      "date : Tue Sep 07 14:57:07 +0000 2021\n",
      "hashtags : []\n",
      "likes : 323\n",
      "retweets : 105\n",
      "url : https://twitter.com/WHO/status/1435255817037664257/photo/1\n",
      "\n",
      "\n",
      "DOC 979 has been retrieved\n",
      "tweet : @DrTedros @Chikwe_I @schwartlanderb @erna_solberg @NAkufoAddo @GlobalGoalsUN @ACTAccelerator \"Your Excellency [Chancellor Merkel] â€“ I dare to call you my friend, You have more than played your part. Your leadership, integrity, humility and dedication are an example to us all\"-@DrTedros\n",
      "username : World Health Organization (WHO)\n",
      "date : Wed Sep 01 14:49:14 +0000 2021\n",
      "hashtags : []\n",
      "likes : 60\n",
      "retweets : 15\n",
      "url : https://twitter.com/WHO/status/1433079505111953414\n",
      "\n",
      "\n",
      "DOC 799 has been retrieved\n",
      "tweet : RT @DrTedros: On behalf of all @WHO staff, I am grateful to @Sept11Memorial for this poignant recognition and the special significance of tâ€¦\n",
      "username : World Health Organization (WHO)\n",
      "date : Sun Sep 12 20:59:05 +0000 2021\n",
      "hashtags : []\n",
      "likes : 0\n",
      "retweets : 64\n",
      "url : https://twitter.com/WHO/status/1437158849518125057\n",
      "\n",
      "\n",
      "DOC 997 has been retrieved\n",
      "tweet : @DrTedros \"Our aim is to put the knowledge and insights that are developed here in Berlin to practical use on the ground all over the ğŸŒ\"-@DrTedros\n",
      "username : World Health Organization (WHO)\n",
      "date : Wed Sep 01 14:28:35 +0000 2021\n",
      "hashtags : []\n",
      "likes : 48\n",
      "retweets : 8\n",
      "url : https://twitter.com/WHO/status/1433074311888347142\n",
      "\n",
      "\n",
      "DOC 978 has been retrieved\n",
      "tweet : Media briefing on the new WHO Hub for pandemic &amp; epidemic intelligence with @DrTedros https://t.co/5Wi9Tr4GXL\n",
      "username : World Health Organization (WHO)\n",
      "date : Wed Sep 01 14:49:47 +0000 2021\n",
      "hashtags : []\n",
      "likes : 470\n",
      "retweets : 152\n",
      "url : https://twitter.com/WHO/status/1433079645042212870\n",
      "\n",
      "\n",
      "DOC 798 has been retrieved\n",
      "tweet : Today is #WorldSepsisDay\n",
      "\n",
      "Q: What is #sepsis?\n",
      "A: Sepsis is a life-threatening medical emergency. It occurs when a person has an infection, and the bodyâ€™s response causes injury to its own tissues and organs. \n",
      "Sepsis affects millions of patients ğŸŒ\n",
      "\n",
      "https://t.co/ejYtk5W1Nn https://t.co/oVpPRz1lsl\n",
      "username : World Health Organization (WHO)\n",
      "date : Mon Sep 13 06:40:14 +0000 2021\n",
      "hashtags : [{'text': 'WorldSepsisDay', 'indices': [9, 24]}, {'text': 'sepsis', 'indices': [37, 44]}]\n",
      "likes : 1850\n",
      "retweets : 1022\n",
      "url : https://twitter.com/WHO/status/1437305100515557379/photo/1\n",
      "\n",
      "\n",
      "DOC 879 has been retrieved\n",
      "tweet : RT @OMS_Afrique: Une flambÃ©e de #Meningite a Ã©tÃ© dÃ©clarÃ©e dans la province de la Tshopo en #RDCğŸ‡¨ğŸ‡©. 261 cas suspects &amp; 129 dÃ©cÃ¨s ont Ã©tÃ© sigâ€¦\n",
      "username : World Health Organization (WHO)\n",
      "date : Wed Sep 08 09:57:54 +0000 2021\n",
      "hashtags : [{'text': 'Meningite', 'indices': [32, 42]}, {'text': 'RDC', 'indices': [91, 95]}]\n",
      "likes : 0\n",
      "retweets : 21\n",
      "url : https://twitter.com/WHO/status/1435542906400645123\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-14-83d1c0f9cb92>:40: DeprecationWarning: Call to deprecated `word_vec` (Use get_vector instead).\n",
      "  embeddings.append(model.wv.word_vec(token))\n"
     ]
    }
   ],
   "source": [
    "top_20(\"Covid EspaÃ±a ola\")\n",
    "#top_20(\"how many covid cases\")\n",
    "#top_20(\"mortalidad covid\")\n",
    "#top_20(\"Covid prevention\")\n",
    "#top_20(\"Pandemia mundial\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
